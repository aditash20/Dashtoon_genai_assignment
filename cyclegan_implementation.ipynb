{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":75676,"sourceType":"datasetVersion","datasetId":42780},{"sourceId":3956508,"sourceType":"datasetVersion","datasetId":850761}],"dockerImageVersionId":30034,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Notebook Content\n1. Importing Necessary Libraries\n1. Loading Data\n1. Building Generator Model\n1. Building Discriminator Model\n1. Building CycleGAN Model\n1. Defining Optimizers and Loss Functions\n1. Training Model\n1. Generating Some Paintings\n1. Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Importing Necessary Libraries\nIn this section I am going to import libraries that I will use.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport cv2 \nfrom glob import glob\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset\nIn this section I am going to load images.","metadata":{}},{"cell_type":"code","source":"main_path = \"../input/van-gogh-paintings\"\nstyle_img_paths = []\nfor class_path in [os.path.join(main_path,class_name) for class_name in os.listdir(main_path)]:\n    \n    class_img_paths = glob(class_path+\"/*\")\n    for class_img_path in class_img_paths:\n        style_img_paths.append(class_img_path)\n\nprint(\"There are {} style images in Van Gogh Paintings Dataset\".format(len(style_img_paths)))","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_path = \"../input/natural-images/natural_images\"\nnormal_image_paths = []\nfor class_path in [os.path.join(main_path,class_name) for class_name in os.listdir(main_path)]:\n    \n    class_img_paths = glob(class_path+\"/*\")\n    for class_img_path in class_img_paths:\n        normal_image_paths.append(class_img_path)\n\nprint(\"There are {} natural images in the Natural Images Dataset\".format(len(normal_image_paths)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We've determined paths of images, now we can read them.","metadata":{}},{"cell_type":"code","source":"style_images = []\nnormal_images = []\n\nfor style_path in style_img_paths:\n    img = cv2.imread(style_path)\n    img = cv2.resize(img,(128,128))\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    style_images.append(img)\n    \nfor normal_path in normal_image_paths:\n    img = cv2.imread(normal_path)\n    img = cv2.resize(img,(128,128))\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    normal_images.append(img)\n    \nprint(len(style_images))\nprint(len(normal_images))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting to float32 \nstyle_images = np.array(style_images,dtype=np.float32)\nnormal_images = np.array(normal_images,dtype=np.float32)\n\n# scaling between -1 and 1\nstyle_images = style_images / 127.5 - 1\nnormal_images = normal_images / 127.5 - 1\n\n# batching\nstyle_images = tf.data.Dataset.from_tensor_slices(style_images).batch(1)\nnormal_images = tf.data.Dataset.from_tensor_slices(normal_images).batch(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Our images are ready, let's check them.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.title(\"Style Images\")\nfor i,image in enumerate(style_images.shuffle(10000).take(16)):\n    plt.subplot(4,4,i+1)\n    plt.imshow(image[0])\n    plt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.title(\"Natural Images\")\nfor i,image in enumerate(normal_images.shuffle(10000).take(16)):\n    plt.subplot(4,4,i+1)\n    plt.imshow(image[0])\n    plt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Generator Model\nIn this section I am going to build the generator networks of the CycleGAN.","metadata":{}},{"cell_type":"code","source":"# we need to downsample and upsample the images, so let's write two new layers, Upsample layer and Downsample Layer\nOUTPUT_CHANNELS = len([\"Red\",\"Green\",\"Blue\"])\n\ndef downsample(filters,size,apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0.,0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02)\n    \n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters,size,strides=2,padding=\"same\",kernel_initializer=initializer,use_bias=False))\n    \n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n    \n    result.add(layers.LeakyReLU())\n    \n    return result\n\ndef upsample(filters,size,apply_dropout=False):\n    initializer = tf.random_normal_initializer(0.,0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02)\n    \n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters,size,strides=2,padding=\"same\",kernel_initializer=initializer,use_bias=False))\n    \n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n    \n    \n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n    \n    result.add(layers.ReLU())\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Thanks to this layers, we can easily create our generator and discriminator.","metadata":{}},{"cell_type":"code","source":"# discriminator\n\ndef Generator():\n    inputs = layers.Input([128,128,3])\n    \n    down_stack = [downsample(128,4), # 64x64x128\n                  downsample(256,4), # 32x32x256\n                  downsample(512,4), # 16x16x512\n                  downsample(512,4), # 8x8x512\n                  downsample(512,4), # 4x4x512\n                  downsample(512,4), # 2x2x512\n                  downsample(512,4), # 1x1x512\n                 ]\n    \n    up_stack = [upsample(512,4,apply_dropout=True), # 2x2\n                upsample(512,4,apply_dropout=True), # 4x4\n                upsample(512,4), # 8x8\n                upsample(256,4), # 16x16\n                upsample(128,4), # 32x32\n                upsample(64,4),  # 64x64\n               ]\n    \n    initializer = tf.random_normal_initializer(0.,0.02)\n    last = last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh')\n    \n    # we'll create skip connections like a residual network\n    x = inputs\n    \n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n        \n    skips = reversed(skips[:-1])\n    \n    for up,skip in zip(up_stack,skips):\n        x = up(x)\n        x = layers.Concatenate()([x,skip])\n        \n    x = last(x)\n    \n    return keras.Model(inputs=inputs,outputs=x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Generator is a bit confusing, I know.\n1. First, generator took 128x128x3 image (this will be our natural image) and converted it into 1x1x512 vector.\n1. Then, upsampled that vector to 128x128x3\n1. We created skip connections, they will prevent vanishing gradient problem.\n1. And finally we returned our model.","metadata":{}},{"cell_type":"markdown","source":"# Building Discriminator Model\nIn this section I am going to build the discriminator model. This will be easier, because Discriminator is a CNN based classifier. It determines whether image is real or generated.\n","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0.,0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02)\n    \n    inp = layers.Input([128,128,3],name=\"input_image\")\n    \n    x = inp\n    \n    down1 = downsample(64,4,False)(x) # 64x64x64\n    down2 = downsample(128,4,False)(down1) # 32x32x128\n    \n    zero_pad1 = layers.ZeroPadding2D()(down2)\n    \n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n    \n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n    \n    leaky_relu = layers.LeakyReLU()(norm1)\n    \n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n    \n    last = layers.Conv2D(1,4,strides=1,kernel_initializer=initializer)(zero_pad2)\n    \n    return keras.Model(inputs=inp,outputs=last)\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Our functions are ready, now we can create our generators and discriminators.","metadata":{}},{"cell_type":"code","source":"vangogh_generator = Generator() # generates van gogh paintings using natural images\nphoto_generator = Generator() # generates natural images using van gogh paintings\n\nvangogh_discriminator = Discriminator() # determines whether van gogh painting is real or generated\nphoto_discriminator = Discriminator() # determines whether natural image is real or generated","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building CycleGAN Model\nIn this section I am going to build CycleGAN model using our generators and discriminators.\n\nI will create a new class by inheriting the model class.","metadata":{}},{"cell_type":"code","source":"class CycleGAN(keras.Model):\n    \n    def __init__(self,\n                 vangogh_generator,\n                 photo_generator,\n                 vangogh_discriminator,\n                 photo_discriminator,\n                 lambda_cycle = 10,\n                ):\n        super(CycleGAN,self).__init__()\n        self.v_gen = vangogh_generator\n        self.p_gen = photo_generator\n        self.v_disc = vangogh_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    \n    def compile(self,\n                v_gen_optimizer,\n                p_gen_optimizer,\n                p_disc_optimizer,\n                gen_loss_fn,\n                disc_loss_fn,\n                cycle_loss_fn,\n                identity_loss_fn,\n                v_disc_optimizer\n               ):\n        super(CycleGAN,self).compile()\n        self.v_gen_optimizer = v_gen_optimizer\n        self.p_gen_optimizer =  p_gen_optimizer\n        self.v_disc_optimizer  = v_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        self.p_disc_optimizer = p_disc_optimizer\n        \n    \n    def train_step(self,batch_data):\n        real_vangogh,real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to vangogh back to photo\n            fake_vangogh = self.v_gen(real_photo,training=True)\n            cycled_photo = self.p_gen(fake_vangogh,training=True)\n            \n            # vangogh to photo back to vangogh\n            fake_photo = self.p_gen(real_vangogh,training=True)\n            cycled_vangogh = self.v_gen(fake_photo,training=True)\n            \n            # generating itself\n            same_photo = self.p_gen(real_photo,training=True)\n            same_vangogh = self.v_gen(real_vangogh,training=True)\n            \n            # checking images using discriminator\n            disc_real_vangogh = self.v_disc(real_vangogh,training=True)\n            disc_real_photo = self.p_disc(real_photo,training=True)\n            \n            disc_fake_vangogh = self.v_disc(fake_vangogh,training=True)\n            disc_fake_photo = self.p_disc(fake_photo,training=True)\n            \n            # computing generator loss\n            vangogh_gen_loss = self.gen_loss_fn(disc_fake_vangogh)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n            \n            # computing total cycle loss\n            total_cycle_loss = self.cycle_loss_fn(real_vangogh,cycled_vangogh,self.lambda_cycle) + self.cycle_loss_fn(real_photo,cycled_photo,self.lambda_cycle)\n            \n            # computing total loss\n            total_vangogh_gen_loss = vangogh_gen_loss + total_cycle_loss + self.identity_loss_fn(real_vangogh, same_vangogh, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n            \n            # computing discriminator loss\n            vangogh_disc_loss = self.disc_loss_fn(disc_real_vangogh,disc_fake_vangogh)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo,disc_fake_photo)\n            \n        \n        # calculating gradients of networks\n        vangogh_generator_gradients = tape.gradient(total_vangogh_gen_loss,self.v_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,self.p_gen.trainable_variables)\n        \n        vangogh_discriminator_gradients = tape.gradient(vangogh_disc_loss,self.v_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,self.p_disc.trainable_variables)\n        \n        self.v_gen_optimizer.apply_gradients(zip(vangogh_generator_gradients,self.v_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,self.p_gen.trainable_variables))\n        \n        self.v_disc_optimizer.apply_gradients(zip(vangogh_discriminator_gradients,\n                                                  self.v_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"v_gen_loss\": total_vangogh_gen_loss,\n            \"p_gen_loss\": total_photo_gen_loss,\n            \"v_disc_loss\": vangogh_disc_loss,\n            \"p_disc_loss\": photo_disc_loss\n        }\n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I know, training looks a bit confusing but don't worry, I will explain\n1. First we created fake photos, same photos, and cycled photos, we'll use them in order to compute loss\n1. Then we computed losses, we'll use these losses in order to compute gradients\n1. Then we computed gradients and applied it to networks.\n","metadata":{}},{"cell_type":"markdown","source":"# Defining Optimizers And Loss Functions\nIn this section I am going to define optimizers and loss functions.","metadata":{}},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\ndef discriminator_loss(real,generated):\n    \n    real_loss = cross_entropy(tf.ones_like(real),real)\n    generated_loss = cross_entropy(tf.zeros_like(generated),generated)\n    \n    total_loss = real_loss + generated_loss\n    \n    return total_loss * 0.5\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In discriminator loss, we've computed real images loss and generated images loss and added them.","metadata":{}},{"cell_type":"code","source":"def generator_loss(generated):\n    \n    return cross_entropy(tf.ones_like(generated),generated)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* But when it comes to compute generator loss, we just use generated (fake) images loss.","metadata":{}},{"cell_type":"code","source":"def cycle_loss(real_image,cycled_image,LAMBDA):\n    \n    return tf.reduce_mean(tf.abs(real_image - cycled_image)) * LAMBDA\n\ndef identity_loss(real_image,same_image,LAMBDA):\n    \n    return tf.reduce_mean(tf.abs(real_image - same_image)) * LAMBDA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll use Adam optimizer\nvangogh_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nphoto_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\nvangogh_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nphoto_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model\nIn this section I am going to train our CycleGAN model.","metadata":{}},{"cell_type":"code","source":"model = CycleGAN(vangogh_generator=vangogh_generator,\n                photo_generator=photo_generator,\n                vangogh_discriminator=vangogh_discriminator,\n                photo_discriminator=photo_discriminator\n                )\n\nmodel.compile(v_gen_optimizer=vangogh_generator_optimizer,\n              p_gen_optimizer=photo_generator_optimizer,\n              p_disc_optimizer=photo_discriminator_optimizer,\n              v_disc_optimizer=vangogh_discriminator_optimizer,\n              gen_loss_fn=generator_loss,\n              disc_loss_fn=discriminator_loss,\n              cycle_loss_fn=cycle_loss,\n              identity_loss_fn=identity_loss\n             )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We created our model, let's fit it for 25 epochs.","metadata":{}},{"cell_type":"code","source":"#if os.path.exists(\"./model.h5\"):\n    #model.load_weights(\"./model.h5\")\n    #print(\"Saved weigths loaded\")\n\n\nmodel.fit(tf.data.Dataset.zip((style_images,normal_images)),\n          epochs=2)\n\nmodel.save_weights(\"model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* I've trained for 4 epoch, each epoch takes 6 minutes.\n* If you try to train 10 epochs, it will be better.","metadata":{}},{"cell_type":"markdown","source":"# Generating Some Paintings\nIn this section I am going to generate some paintings.","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(10,10))\nfor i,image in enumerate(normal_images.shuffle(10000).take(36)):\n    plt.subplot(6,6,i+1)\n    prediction = vangogh_generator(image,training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    plt.imshow(X = prediction)\n    plt.axis(\"off\")\n    if i==36:\n        break\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Our model works right, but it needs to train more.\n","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\nThanks for your attention, if you have any questions in your mind, please ask. I will definetely return.","metadata":{}}]}